{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "ename": "IndentationError",
     "evalue": "unindent does not match any outer indentation level (<tokenize>, line 90)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<tokenize>\"\u001b[0;36m, line \u001b[0;32m90\u001b[0m\n\u001b[0;31m    rewarded_experiences = get_experiences(lambda x: x != 0.0)\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mIndentationError\u001b[0m\u001b[0;31m:\u001b[0m unindent does not match any outer indentation level\n"
     ]
    }
   ],
   "source": [
    "import sys\n",
    "sys.path.append('../supervised/keras')\n",
    "\n",
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "\n",
    "from keras import backend as K\n",
    "from keras.models import load_model, clone_model\n",
    "from keras.utils.np_utils import to_categorical\n",
    "\n",
    "from collections import deque\n",
    "\n",
    "from metrics import fmeasure, recall, precision\n",
    "\n",
    "import gym\n",
    "import gym_air_hockey\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plot_state(state):\n",
    "    state = np.uint8(np.copy(state) * 128 + 128)\n",
    "    f, ax = plt.subplots(1, 3)\n",
    "    ax[0].imshow(state[0:3].transpose((1,2,0)))\n",
    "    ax[1].imshow(state[3:6].transpose((1,2,0)))\n",
    "    ax[2].imshow(state[6:9].transpose((1,2,0)))\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "\n",
    "    K.set_learning_phase(1)\n",
    "\n",
    "    n_steps = 1000000\n",
    "    training_start = 100\n",
    "    training_interval = 10\n",
    "    save_steps = 10000\n",
    "    copy_steps = 1000\n",
    "    discount_rate = 0.993\n",
    "    batch_size = 128\n",
    "    iteration = 0\n",
    "    done = True\n",
    "    default_reset = 1000\n",
    "    replay_memory_size = 2000\n",
    "    eps_min = 0.0\n",
    "    eps_max = 0.0\n",
    "    eps_decay_steps = n_steps * 0.6\n",
    "\n",
    "    env = gym.make('AirHockey-v0')\n",
    "    processor = gym_air_hockey.DataProcessor()\n",
    "\n",
    "    actor = load_model('../supervised/keras/models/model.h5', {'fmeasure': fmeasure, 'recall': recall, 'precision': precision})\n",
    "\n",
    "    critic = clone_model(actor)\n",
    "    critic.set_weights(actor.get_weights())\n",
    "\n",
    "    for actor_weight, critic_weight in zip(actor.get_weights(), critic.get_weights()):\n",
    "        assert np.allclose(actor_weight, critic_weight)\n",
    "\n",
    "    def get_vars(model):\n",
    "        return {var.name: var for var in model.trainable_weights}\n",
    "    actor_q_values = actor.output\n",
    "    critic_q_values = critic.output\n",
    "    actor_vars = get_vars(actor)\n",
    "    critic_vars = get_vars(critic)\n",
    "\n",
    "    X_action = tf.placeholder(tf.int32, shape=[None])\n",
    "    q_value = tf.reduce_sum(actor_q_values * tf.one_hot(X_action, 10), axis=1, keep_dims=True)\n",
    "\n",
    "    y = tf.placeholder(tf.float32, shape=[None, 1])\n",
    "    cost = tf.reduce_mean(tf.square(y - q_value))\n",
    "    global_step = tf.Variable(0, trainable=False, name='global_step')\n",
    "    optimizer = tf.train.AdamOptimizer()\n",
    "    training_op = optimizer.minimize(cost, global_step=global_step)\n",
    "\n",
    "    replay_memory = deque([], maxlen=replay_memory_size)\n",
    "\n",
    "    def sample_memories(batch_size):\n",
    "\n",
    "        def get_experiences(filter_func):\n",
    "            from random import shuffle\n",
    "            experiences = [row for row in replay_memory if filter_func(row['reward'])]\n",
    "            if len(experiences) == 0:\n",
    "                print('Get experiences from replay_memory instead')\n",
    "                experiences = [row for row in replay_memory]\n",
    "            shuffle(experiences)\n",
    "            experiences = experiences[:batch_size]\n",
    "\n",
    "            return experiences\n",
    "\n",
    "        rewarded_experiences = get_experiences(lambda x: x != 0.0)\n",
    "        typical_experiences  = get_experiences(lambda x: x == 0.0)\n",
    "\n",
    "        mixed_experiences = []\n",
    "        for a, b in zip(rewarded_experiences, typical_experiences):\n",
    "            mixed_experiences.append(a)\n",
    "            mixed_experiences.append(b)\n",
    "#\n",
    "##            print('Rewarded: current')\n",
    "##            plot(a['state'])\n",
    "##            print('Rewarded: next')\n",
    "##            plot(a['next_state'])\n",
    "##\n",
    "##            print('Typical: current')\n",
    "##            plot(b['state'])\n",
    "##            print('Typical: next')\n",
    "##            plot(b['next_state'])\n",
    "##        print('Press any button')\n",
    "##        input()\n",
    "\n",
    "        indices = np.random.permutation(len(mixed_experiences))[:batch_size]\n",
    "        cols = [[], [], [], [], []]\n",
    "        for idx in indices:\n",
    "            memory = mixed_experiences[idx]\n",
    "            for col, key in zip(cols, memory):\n",
    "                col.append(memory[key])\n",
    "        cols = [np.array(col) for col in cols]\n",
    "        return cols[0], cols[1], cols[2].reshape(-1, 1), cols[3], cols[4].reshape(-1, 1)\n",
    "\n",
    "\n",
    "    def epsilon_greedy(q_values, step):\n",
    "        epsilon = max(eps_min, eps_max - (eps_max - eps_min) * step / eps_decay_steps)\n",
    "#         if step % 100 == 0:\n",
    "#             print('Epsilon %f' % epsilon)\n",
    "        if np.random.randn() < epsilon:\n",
    "            return np.random.randint(10)\n",
    "        return np.argmax(q_values)\n",
    "\n",
    "    reset = default_reset\n",
    "    sess = K.get_session()\n",
    "    while True:\n",
    "        reset -= 1\n",
    "        step = global_step.eval(session=sess)\n",
    "        iteration += 1\n",
    "\n",
    "        if step > n_steps:\n",
    "            break\n",
    "\n",
    "        if done or not reset:\n",
    "            reset = default_reset\n",
    "            observation = env.reset()\n",
    "            _ = processor.process_observation(observation)\n",
    "            _ = processor.process_observation(observation)\n",
    "            state = np.copy(processor.process_observation(observation))\n",
    "\n",
    "        q_values = actor_q_values.eval(session=sess, feed_dict={actor.input: [state]})\n",
    "        action = epsilon_greedy(q_values, step)\n",
    "\n",
    "        observation, reward, done, info = env.step(processor.process_action(action), dt=2)\n",
    "#         if abs(reward) > 0.0:\n",
    "#             print('Reward %f' % reward)\n",
    "        next_state = processor.process_observation(observation)\n",
    "\n",
    "        replay_memory.append({'state': np.copy(state),\n",
    "                              'action': action,\n",
    "                              'reward': reward,\n",
    "                              'next_state': np.copy(next_state),\n",
    "                              'continue': 1.0 - done})\n",
    "        state = np.copy(next_state)\n",
    "\n",
    "        if iteration < training_start or iteration % training_interval != 0:\n",
    "            continue\n",
    "\n",
    "#         print('Updated %d %d' % (iteration, step))\n",
    "        X_state_val, X_action_val, rewards, X_next_state_val, continues = sample_memories(batch_size)\n",
    "\n",
    "        for s_t, a_t, r_t, s_t_1, c in zip(X_state_val, X_action_val, rewards, X_next_state_val, continues):\n",
    "            if (abs(r_t) > 0.0):\n",
    "                print('Action   %d' % a_t)\n",
    "                print('Reward   %f' % r_t)\n",
    "                print('Continue %d' % c)\n",
    "                plot_state(s_t)\n",
    "                plot_state(s_t_1)\n",
    "\n",
    "        next_q_values = critic_q_values.eval(session=sess, feed_dict={critic.input: X_next_state_val})\n",
    "        max_next_q_values = np.max(next_q_values, axis=1).reshape(-1, 1)\n",
    "        y_val = rewards + continues * discount_rate * max_next_q_values\n",
    "        training_op.run(session=sess, feed_dict={actor.input: X_state_val, X_action: X_action_val, y: y_val})\n",
    "\n",
    "        if step % copy_steps == 0:\n",
    "            critic.set_weights(actor.get_weights())\n",
    "\n",
    "        if step % save_steps == 0:\n",
    "            actor.save('rl_model.h5')\n",
    "\n",
    "\n",
    "    actor.save('rl_model.h5')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true,
    "scrolled": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
